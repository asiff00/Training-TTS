{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLqBa4uYPrqE"
      },
      "source": [
        "### Install packages and download models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H72WF06ZPrTF",
        "outputId": "2cde59a3-aa81-494e-9fdd-fb52a9f7cbea"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/yl4579/StyleTTS2.git\n",
        "cd StyleTTS2\n",
        "pip install Soundfile torchaudio munch torch pydub pyyaml librosa nltk matplotlib accelerate transformers phonemizer einops einops-exts tqdm typing-extensions git+https://github.com/resemble-ai/monotonic_align.git\n",
        "sudo apt-get install espeak-ng\n",
        "git clone https://huggingface.co/yl4579/StyleTTS2-LibriTTS\n",
        "mv StyleTTS2-LibriTTS/Models ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G398sL8wPzTB"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJuQUBrEPy5C",
        "outputId": "fb68b644-20e6-4b1f-dba8-9e4b0d1f7761"
      },
      "outputs": [],
      "source": [
        "%cd StyleTTS2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDXW8ZZePuSb",
        "outputId": "7565e4d4-7f19-4f2e-ac7b-189c5d193dad"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# = Code Cell 3: Load Voice Dataset =\n",
        "# ==========================================\n",
        "\n",
        "# We will download and extract the dataset, then move only the 'female/mono' folder\n",
        "# to the desired location and remove unnecessary files.\n",
        "!rm -rf Data\n",
        "\n",
        "!kaggle datasets download -d mobassir/comprehensive-bangla-tts --unzip -p Data\n",
        "\n",
        "# Move the 'female/mono' folder to 'Data/raw' and remove other files\n",
        "!mkdir -p Data\n",
        "!mv Data/iitm_bangla_tts/comprehensive_bangla_tts/female/mono/* Data/\n",
        "\n",
        "# Rename wav directory to wavs\n",
        "!mv Data/wav Data/wavs\n",
        "\n",
        "# Clean up unnecessary files\n",
        "!rm -rf Data/comprehensive_bangla_tts_weights\n",
        "!rm -rf Data/comprehensive_bangla_tts\n",
        "!rm -rf Data/iitm_bangla_tts\n",
        "!rm -rf Data/vits_m_phoneme\n",
        "!rm Data/license.pdf\n",
        "!rm Data/txt.done.data\n",
        "\n",
        "\n",
        "# The final directory structure should now look like this:\n",
        "# Data/\n",
        "#   ├── wavs/\n",
        "#   └── metadata_female.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_HHzdBZdHAd",
        "outputId": "0329ae80-fd78-4d70-e90b-31bb4a83d76e"
      },
      "outputs": [],
      "source": [
        "# Clean up old files\n",
        "\n",
        "!rm Data/OOD_texts.txt\n",
        "!rm Data/main_list.txt\n",
        "!rm Data/train_list.txt\n",
        "!rm Data/val_list.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDslYPaiYscK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from phonemizer import phonemize\n",
        "\n",
        "metadata_path = \"Data/metadata_female.txt\"\n",
        "main_list_path = \"Data/main_list.txt\"\n",
        "train_list_path = \"Data/train_list.txt\"\n",
        "val_list_path = \"Data/val_list.txt\"\n",
        "ood_file_path = \"Data/OOD_texts.txt\"\n",
        "sample_count = 5000\n",
        "\n",
        "def phonemize_bengali(text):\n",
        "    \"\"\"\n",
        "    Convert Bengali text to phonemes using eSpeak-ng.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return phonemize(\n",
        "            text,\n",
        "            language='bn',\n",
        "            backend='espeak',\n",
        "            strip=True,\n",
        "            preserve_punctuation=True,\n",
        "            with_stress=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error phonemizing text '{text}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if os.path.exists(metadata_path):\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    formatted_lines = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) >= 2:\n",
        "            wav_id = parts[0]\n",
        "            transcription = parts[1]\n",
        "            relative_path = f\"Data/wavs/{wav_id}.wav\"\n",
        "            formatted_lines.append(f\"{relative_path}|{transcription}\")\n",
        "\n",
        "    with open(main_list_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(formatted_lines))\n",
        "\n",
        "    print(f\"Created {main_list_path} with {len(formatted_lines)} entries\")\n",
        "\n",
        "    root_wavs = \"Data/wavs\"\n",
        "    broken_files = []\n",
        "    missing_files = []\n",
        "\n",
        "    for line in formatted_lines:\n",
        "        parts = line.split(\"|\")\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        wav_path = parts[0].strip()\n",
        "\n",
        "        if not os.path.exists(wav_path):\n",
        "            missing_files.append(wav_path)\n",
        "            continue\n",
        "        try:\n",
        "            import soundfile as sf\n",
        "            data, sr = sf.read(wav_path)\n",
        "            if len(data) == 0:\n",
        "                broken_files.append((wav_path, \"Empty/0 Samples\"))\n",
        "        except Exception as e:\n",
        "            broken_files.append((wav_path, str(e)))\n",
        "\n",
        "    print(\"Missing files:\", len(missing_files))\n",
        "    for m in missing_files[:10]:\n",
        "        print(\"  -\", m)\n",
        "\n",
        "    print(\"Defective/unreadable files:\", len(broken_files))\n",
        "    for b in broken_files[:10]:\n",
        "        print(\"  -\", b[0], \"| Error:\", b[1])\n",
        "\n",
        "    if not os.path.exists(ood_file_path):\n",
        "        if len(formatted_lines) >= sample_count:\n",
        "            sampled_lines = random.sample(formatted_lines, sample_count)\n",
        "            phonemized_lines = []\n",
        "            for line in sampled_lines:\n",
        "                parts = line.split('|')\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "                wav_path, text = parts\n",
        "                phonemes = phonemize_bengali(text)\n",
        "                if phonemes:\n",
        "                    phonemized_lines.append(f\"{wav_path}|{phonemes}|0\")\n",
        "\n",
        "            with open(ood_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write('\\n'.join(phonemized_lines))\n",
        "\n",
        "            print(f\"Created {ood_file_path} with {len(phonemized_lines)} phonemized entries\")\n",
        "        else:\n",
        "            print(f\"Warning: Not enough entries to sample {sample_count} items\")\n",
        "\n",
        "    if os.path.exists(ood_file_path):\n",
        "        with open(ood_file_path, 'r', encoding='utf-8') as f:\n",
        "            ood_lines = f.readlines()\n",
        "        ood_lines = [line.strip() for line in ood_lines if line.strip()]\n",
        "        total_ood = len(ood_lines)\n",
        "        train_samples = int(total_ood * 0.8)\n",
        "        eval_samples = total_ood - train_samples\n",
        "\n",
        "        train_lines = []\n",
        "        val_lines = []\n",
        "        for line in ood_lines[:train_samples]:\n",
        "            parts = line.split('|')\n",
        "            if len(parts) == 3:\n",
        "                filename = os.path.basename(parts[0])\n",
        "                train_lines.append(f\"{filename}|{parts[1]}|0\")\n",
        "        for line in ood_lines[train_samples:train_samples + eval_samples]:\n",
        "            parts = line.split('|')\n",
        "            if len(parts) == 3:\n",
        "                filename = os.path.basename(parts[0])\n",
        "                val_lines.append(f\"{filename}|{parts[1]}|0\")\n",
        "\n",
        "        with open(train_list_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(train_lines))\n",
        "\n",
        "        with open(val_list_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(val_lines))\n",
        "\n",
        "        print(f\"Using {ood_file_path} for training and validation\")\n",
        "        print(f\"Created {train_list_path} with {len(train_lines)} entries\")\n",
        "        print(f\"Created {val_list_path} with {len(val_lines)} entries\")\n",
        "    else:\n",
        "        print(f\"Error: OOD file {ood_file_path} not found\")\n",
        "else:\n",
        "    print(f\"Error: Metadata file {metadata_path} not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlBQREWU8ud"
      },
      "source": [
        "### Change the finetuning config\n",
        "\n",
        "Depending on the GPU you got, you may want to change the bacth size, max audio length, epiochs and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uEITi0hU4I2"
      },
      "outputs": [],
      "source": [
        "config_path = \"Configs/config_ft.yml\"\n",
        "import yaml\n",
        "config = yaml.safe_load(open(config_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TPTRgOKSVT4K"
      },
      "outputs": [],
      "source": [
        "config['data_params']['root_path'] = \"Data/wavs\"\n",
        "config['data_params']['train'] = \"Data/wavs\"\n",
        "config['batch_size'] = 8\n",
        "config['epochs'] = 500\n",
        "config['max_len'] = 128\n",
        "config['pretrained_model'] = \"Models/LibriTTS/epochs_2nd_00020.pth\"\n",
        "config['loss_params']['joint_epoch'] = 512\n",
        "\n",
        "\n",
        "\n",
        "with open(config_path, 'w') as outfile:\n",
        "  yaml.dump(config, outfile, default_flow_style=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUuB_19NWj2Y"
      },
      "source": [
        "### Start finetuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZVAD5GKWm-O",
        "outputId": "132d5723-ba75-4c2e-c091-85d53fddba9b"
      },
      "outputs": [],
      "source": [
        "!python train_finetune_accelerate.py --config_path ./Configs/config_ft.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0_7wsGkXGfc"
      },
      "source": [
        "### Test the model quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPLphjbncE7p"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIIAoDACXJL0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# load packages\n",
        "import time\n",
        "import random\n",
        "import yaml\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from models import *\n",
        "from utils import *\n",
        "from text_utils import TextCleaner\n",
        "textclenaer = TextCleaner()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def length_to_mask(lengths):\n",
        "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
        "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
        "    return mask\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def compute_style(path):\n",
        "    wave, sr = librosa.load(path, sr=24000)\n",
        "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "    if sr != 24000:\n",
        "        audio = librosa.resample(audio, sr, 24000)\n",
        "    mel_tensor = preprocess(audio).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
        "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
        "\n",
        "    return torch.cat([ref_s, ref_p], dim=1)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# load phonemizer\n",
        "import phonemizer\n",
        "global_phonemizer = phonemizer.backend.EspeakBackend(language='bn', preserve_punctuation=True,  with_stress=True)\n",
        "\n",
        "config = yaml.safe_load(open(\"Models/LJSpeech/config_ft.yml\"))\n",
        "\n",
        "# load pretrained ASR model\n",
        "ASR_config = config.get('ASR_config', False)\n",
        "ASR_path = config.get('ASR_path', False)\n",
        "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
        "\n",
        "# load pretrained F0 model\n",
        "F0_path = config.get('F0_path', False)\n",
        "pitch_extractor = load_F0_models(F0_path)\n",
        "\n",
        "# load BERT model\n",
        "from Utils.PLBERT.util import load_plbert\n",
        "BERT_path = config.get('PLBERT_dir', False)\n",
        "plbert = load_plbert(BERT_path)\n",
        "\n",
        "model_params = recursive_munch(config['model_params'])\n",
        "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
        "_ = [model[key].eval() for key in model]\n",
        "_ = [model[key].to(device) for key in model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKXRAyyzcMpQ"
      },
      "outputs": [],
      "source": [
        "files = [f for f in os.listdir(\"Models/LJSpeech/\") if f.endswith('.pth')]\n",
        "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULuU9-VDb9Pk"
      },
      "outputs": [],
      "source": [
        "params_whole = torch.load(\"Models/LJSpeech/\" + sorted_files[-1], map_location='cpu')\n",
        "params = params_whole['net']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-U29yIYc2ea"
      },
      "outputs": [],
      "source": [
        "for key in model:\n",
        "    if key in params:\n",
        "        print('%s loaded' % key)\n",
        "        try:\n",
        "            model[key].load_state_dict(params[key])\n",
        "        except:\n",
        "            from collections import OrderedDict\n",
        "            state_dict = params[key]\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:]\n",
        "                new_state_dict[name] = v\n",
        "\n",
        "            model[key].load_state_dict(new_state_dict, strict=False)\n",
        "_ = [model[key].eval() for key in model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2CWYNoqc455"
      },
      "outputs": [],
      "source": [
        "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
        "sampler = DiffusionSampler(\n",
        "    model.diffusion.diffusion,\n",
        "    sampler=ADPM2Sampler(),\n",
        "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
        "    clamp=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x5kVb3nc_eY"
      },
      "outputs": [],
      "source": [
        "def inference(text, ref_s, alpha = 0.3, beta = 0.7, diffusion_steps=5, embedding_scale=1):\n",
        "    text = text.strip()\n",
        "    ps = global_phonemizer.phonemize([text])\n",
        "    ps = word_tokenize(ps[0])\n",
        "    ps = ' '.join(ps)\n",
        "    tokens = textclenaer(ps)\n",
        "    tokens.insert(0, 0)\n",
        "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
        "        text_mask = length_to_mask(input_lengths).to(device)\n",
        "\n",
        "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "\n",
        "        s_pred = sampler(noise = torch.randn((1, 256)).unsqueeze(1).to(device),\n",
        "                                          embedding=bert_dur,\n",
        "                                          embedding_scale=embedding_scale,\n",
        "                                            features=ref_s,\n",
        "                                             num_steps=diffusion_steps).squeeze(1)\n",
        "\n",
        "\n",
        "        s = s_pred[:, 128:]\n",
        "        ref = s_pred[:, :128]\n",
        "\n",
        "        ref = alpha * ref + (1 - alpha)  * ref_s[:, :128]\n",
        "        s = beta * s + (1 - beta)  * ref_s[:, 128:]\n",
        "\n",
        "        d = model.predictor.text_encoder(d_en,\n",
        "                                         s, input_lengths, text_mask)\n",
        "\n",
        "        x, _ = model.predictor.lstm(d)\n",
        "        duration = model.predictor.duration_proj(x)\n",
        "\n",
        "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
        "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
        "\n",
        "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
        "        c_frame = 0\n",
        "        for i in range(pred_aln_trg.size(0)):\n",
        "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
        "            c_frame += int(pred_dur[i].data)\n",
        "\n",
        "        # encode prosody\n",
        "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        if model_params.decoder.type == \"hifigan\":\n",
        "            asr_new = torch.zeros_like(en)\n",
        "            asr_new[:, :, 0] = en[:, :, 0]\n",
        "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
        "            en = asr_new\n",
        "\n",
        "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "\n",
        "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        if model_params.decoder.type == \"hifigan\":\n",
        "            asr_new = torch.zeros_like(asr)\n",
        "            asr_new[:, :, 0] = asr[:, :, 0]\n",
        "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
        "            asr = asr_new\n",
        "\n",
        "        out = model.decoder(asr,\n",
        "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "\n",
        "\n",
        "    return out.squeeze().cpu().numpy()[..., :-50] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O159JnwCc6CC"
      },
      "source": [
        "### Synthesize speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThciXQ6rc9Eq"
      },
      "outputs": [],
      "source": [
        "text = '''হ্যালো আপনি কেমন আছেন? সব ভাল তো? আসুন গল্প করি।'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jldPkJyCc83a"
      },
      "outputs": [],
      "source": [
        "# get a random reference in the training set, note that it doesn't matter which one you use\n",
        "path = \"/content/StyleTTS2/Data/wavs/train_bengalifemale_00117.wav\"\n",
        "# this style vector ref_s can be saved as a parameter together with the model weights\n",
        "ref_s = compute_style(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mIU0jqDdQ-c"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "wav = inference(text, ref_s, alpha=0.9, beta=0.9, diffusion_steps=10, embedding_scale=1)\n",
        "rtf = (time.time() - start) / (len(wav) / 24000)\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "import IPython.display as ipd\n",
        "display(ipd.Audio(wav, rate=24000, normalize=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Audio",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
