{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQxa8RG33crc"
      },
      "outputs": [],
      "source": [
        "!pip install coqui-tts -U -q\n",
        "!pip install install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fysmONas2pAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "from trainer import Trainer, TrainerArgs\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits, VitsAudioConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.tts.configs.shared_configs import (\n",
        "    BaseDatasetConfig,\n",
        "    CharactersConfig,\n",
        ")\n",
        "from TTS.tts.datasets import load_tts_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFRCgHIh2sAk"
      },
      "outputs": [],
      "source": [
        "### Vits takes singificatly longer time to train than style-tts2 \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 104\n",
        "EPOCHS = 1000\n",
        "MAX_SAMPLES = 5000\n",
        "VAL_SPLIT = 0.1\n",
        "LOG_FILE = \"training.log\"\n",
        "male = False\n",
        "pretrained = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CycXFDEj2vj0"
      },
      "outputs": [],
      "source": [
        "\n",
        "pretrained_path = \"\"\n",
        "if pretrained:\n",
        "    pretrained_path = \"./output/vits/\"\n",
        "if male:\n",
        "    meta_file = \"./content/drive/MyDrive/mono/metadata_male.txt\"\n",
        "    root_path = \"/content/drive/MyDrive/mono\"\n",
        "else:\n",
        "    meta_file = \"/content/drive/MyDrive/mono/metadata_female.txt\"\n",
        "    root_path = \"/content/drive/MyDrive/mono\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOG_FILE),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger('bangla_tts_training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "481XZ8362zZF"
      },
      "outputs": [],
      "source": [
        "def formatter(root_path, meta_file, **kwargs):\n",
        "    \"\"\"\n",
        "    Normalizes the LJSpeech meta data file to TTS format.\n",
        "    Validates characters in each line and reports problematic entries.\n",
        "    \"\"\"\n",
        "    txt_file = meta_file\n",
        "    items = []\n",
        "    speaker_name = \"ljspeech\"\n",
        "    skipped_lines = 0\n",
        "\n",
        "    logger.info(f\"Reading metadata from {meta_file}\")\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
        "        for line_num, line in enumerate(ttf, 1):\n",
        "            try:\n",
        "                cols = line.split(\"|\")\n",
        "                if len(cols) < 2:\n",
        "                    logger.warning(f\"Line {line_num} has invalid format: {line}\")\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "\n",
        "                wav_file = os.path.join(root_path, \"wav\", cols[0] + \".wav\")\n",
        "                if not os.path.exists(wav_file):\n",
        "                    logger.warning(f\"Line {line_num}: WAV file not found: {wav_file}\")\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "\n",
        "                text = cols[1].strip()\n",
        "                if not text:\n",
        "                    logger.warning(f\"Line {line_num}: Empty text for file {cols[0]}\")\n",
        "                    skipped_lines += 1\n",
        "                    continue\n",
        "\n",
        "                items.append(\n",
        "                    {\n",
        "                        \"text\": text,\n",
        "                        \"audio_file\": wav_file,\n",
        "                        \"speaker_name\": speaker_name,\n",
        "                        \"root_path\": root_path,\n",
        "                    }\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Line {line_num}: Error processing line: {e}\")\n",
        "                skipped_lines += 1\n",
        "\n",
        "    if skipped_lines > 0:\n",
        "        logger.warning(f\"Skipped {skipped_lines} lines in metadata file due to errors\")\n",
        "\n",
        "    logger.info(f\"Successfully loaded {len(items)} items from metadata\")\n",
        "    return items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = \"./output\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "logger.info(f\"Output path: {output_path}\")\n",
        "\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    meta_file_train=meta_file,\n",
        "    path=os.path.join(root_path, \"\")\n",
        ")\n",
        "logger.info(f\"Dataset config: {dataset_config}\")\n",
        "\n",
        "eval_split_size = VAL_SPLIT\n",
        "eval_split_max_size = int(MAX_SAMPLES * VAL_SPLIT)\n",
        "logger.info(f\"Using validation split: {eval_split_size}, max validation samples: {eval_split_max_size}\")\n",
        "\n",
        "logger.info(f\"Loading samples from {meta_file}\")\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    formatter=formatter,\n",
        "    eval_split=True,\n",
        "    eval_split_size=eval_split_size,\n",
        "    eval_split_max_size=eval_split_max_size\n",
        ")\n",
        "\n",
        "\n",
        "if len(train_samples) > MAX_SAMPLES - len(eval_samples):\n",
        "    logger.info(f\"Limiting training samples from {len(train_samples)} to {MAX_SAMPLES - len(eval_samples)}\")\n",
        "    train_samples = train_samples[:MAX_SAMPLES - len(eval_samples)]\n",
        "\n",
        "logger.info(f\"Training samples: {len(train_samples)}, Validation samples: {len(eval_samples)}\")\n",
        "logger.debug(f\"Sample example: {train_samples[0]}\")\n",
        "logger.info(\"Analyzing training data for character coverage...\")\n",
        "all_train_texts = [sample['text'] for sample in train_samples + eval_samples]\n",
        "unique_chars = set()\n",
        "for text in all_train_texts:\n",
        "    unique_chars.update(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJydxpZW2fpd",
        "outputId": "35f5b177-ba9f-456e-ab63-6fd34c37a8ec"
      },
      "outputs": [],
      "source": [
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=22050,\n",
        "    win_length=1024,\n",
        "    hop_length=256,\n",
        "    num_mels=80,\n",
        "    mel_fmin=0,\n",
        "    mel_fmax=None,\n",
        ")\n",
        "logger.info(\"Audio configuration created\")\n",
        "\n",
        "bangla_chars = \"অআইঈউঊঋঌএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহড়ঢ়য়ৎংঃঁ০১২৩৪৫৬৭৮৯ািীুূৃৄেৈোৌ্ৗৢৣ—\"\n",
        "additional_chars = \"$%&''\\\"`\\\"„‘’\"\n",
        "\n",
        "if male:\n",
        "    logger.info(\"Using male character configuration with comprehensive Bangla set and common punctuation\")\n",
        "    characters_config = CharactersConfig(\n",
        "        pad=\"<PAD>\",\n",
        "        eos=\"।\",  # '<EOS>', #'।',\n",
        "        bos=\"<BOS>\",  # None,\n",
        "        blank=\"<BLNK>\",\n",
        "        phonemes=None,\n",
        "        characters=bangla_chars + \"''\\u200c\\u200d\" + additional_chars,\n",
        "        punctuations=\"!,.:;?()- |\",\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Using female character configuration with comprehensive Bangla set and common punctuation\")\n",
        "    characters_config = CharactersConfig(\n",
        "        pad=\"<PAD>\",\n",
        "        eos=\"।\",  # '<EOS>', #'।',\n",
        "        bos=\"<BOS>\",  # None,\n",
        "        blank=\"<BLNK>\",\n",
        "        phonemes=None,\n",
        "        characters=bangla_chars + \"''\\u200c\\u200d\" + additional_chars,\n",
        "        punctuations=\"!,.:;?()- |\",\n",
        "    )\n",
        "\n",
        "#test sentences\n",
        "test_sentences = [\n",
        "    \"হয়,হয়ে,ওয়া,হয়েছ,হয়েছে,দিয়ে,যায়,দায়,নিশ্চয়,আয়,ভয়,নয়,আয়াত,নিয়ে,হয়েছে,দিয়েছ,রয়ে,রয়েছ,রয়েছে।\",\n",
        "    \"দেয়,দেওয়া,বিষয়,হয়,হওয়া,সম্প্রদায়,সময়,হয়েছি,দিয়েছি,হয়,হয়েছিল,বিষয়ে,নয়,কিয়াম,ইয়া,দেয়া,দিয়েছে,আয়াতে,দয়া।\",\n",
        "    \"ইয়াহুদ,নয়,ব্যয়,ইয়াহুদী,নেওয়া,উভয়ে,যায়,হয়েছিল,প্রয়োজন।\",\n",
        "]\n",
        "\n",
        "# VITS model configuration\n",
        "run_name = f\"vits_{datetime.now().strftime('%b_%d')}\"\n",
        "logger.info(f\"Creating model configuration with run name: {run_name}\")\n",
        "config = VitsConfig(\n",
        "    audio=audio_config,\n",
        "    run_name=run_name,\n",
        "    use_speaker_embedding=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    eval_batch_size=BATCH_SIZE,\n",
        "    batch_group_size=0,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=EPOCHS,\n",
        "    text_cleaner=\"multilingual_cleaners\",\n",
        "    use_phonemes=False,\n",
        "    phoneme_language=\"bn\",\n",
        "    compute_input_seq_cache=True,\n",
        "    print_step=50,\n",
        "    print_eval=True,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    characters=characters_config,\n",
        "    save_step=1000,\n",
        "    cudnn_benchmark=True,\n",
        "    cudnn_deterministic=True,\n",
        "    eval_split_size=eval_split_size,\n",
        "    eval_split_max_size=eval_split_max_size,\n",
        "    test_sentences=test_sentences,\n",
        ")\n",
        "\n",
        "config_path = os.path.join(output_path, \"config.json\")\n",
        "config.save_json(config_path)\n",
        "logger.info(f\"Configuration saved to {config_path}\")\n",
        "\n",
        "logger.info(\"Initializing audio processor\")\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "logger.info(f\"Audio processor resample: {ap.resample}\")\n",
        "\n",
        "logger.info(\"Initializing tokenizer\")\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "logger.info(f\"Creating VITS model with device: {device}\")\n",
        "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
        "model = model.to(device)\n",
        "\n",
        "logger.info(\"Setting up trainer\")\n",
        "trainer_args = TrainerArgs(continue_path=pretrained_path)\n",
        "trainer = Trainer(\n",
        "    trainer_args,\n",
        "    config,\n",
        "    output_path,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYrASEV025l2",
        "outputId": "259ccd33-59dd-4399-c6c0-b1c0d5f6e784"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Starting training\")\n",
        "try:\n",
        "        trainer.fit()\n",
        "        logger.info(\"Training completed successfully\")\n",
        "except Exception as e:\n",
        "        logger.error(f\"Training failed with error: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YEl18HH_LQU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits, VitsAudioConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np\n",
        "\n",
        "config_path = \"/content/output/vits/config.json\" \n",
        "config = VitsConfig()\n",
        "config.load_json(config_path)\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
        "model.load_checkpoint(config, checkpoint_path=\"/content/output/vits/best_model_10082.pth\", eval=True)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "text =\"\"\"একটি ছোট্ট গ্রামে বাস করত রমেশ নামের এক কৃষক। সে খুবই গরিব ছিল, কিন্তু তার বুদ্ধি ছিল অসাধারণ। একদিন রমেশের জমিতে একটি বড়ো কুমড়া ফলল। এত বড়ো কুমড়া সে আগে কখনো দেখেনি!\n",
        "\n",
        "রমেশ ভাবল, \"এটি রাজাকে উপহার দিলে যদি কিছু পুরস্কার পাই!\" তাই সে কুমড়াটি নিয়ে রাজপ্রাসাদে গেল।\n",
        "\n",
        "রাজা খুবই খুশি হলেন এবং বললেন, \"তোমাকে আমি পুরস্কার দেব। তুমি কী চাও?\"\n",
        "\n",
        "রমেশ বলল, \"মহারাজ, আমি কিছু চাই না। আপনি খুশি থাকলেই আমি খুশি।\"\n",
        "\n",
        "রাজা তার বিনয় দেখে খুশি হয়ে তাকে অনেক স্বর্ণমুদ্রা দিলেন।\n",
        "\n",
        "এ ঘটনা শুনে গ্রামের এক ধনী লোভী ব্যবসায়ী ভাবল, \"যদি কুমড়া উপহার দিয়ে এত পুরস্কার পাওয়া যায়, তাহলে আমি রাজাকে ঘোড়া উপহার দেব, নিশ্চয়ই অনেক কিছু পাব!\"\n",
        "\n",
        "সে এক দুর্লভ ঘোড়া কিনে রাজাকে উপহার দিল।\n",
        "\n",
        "রাজা তখন হেসে বললেন, \"তুমি আমাকে এত সুন্দর ঘোড়া উপহার দিয়েছো, আমি তোমাকে সেই কৃষকের দেওয়া বড় কুমড়াটি উপহার দিচ্ছি!\"\n",
        "\n",
        "ধনী ব্যবসায়ী হতভম্ভ হয়ে গেল। সে বুঝতে পারল, শুধু লোভ করলে সবকিছু পাওয়া যায় না, বুদ্ধি আর বিনয়ই প্রকৃত সম্পদ।\"\"\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer.text_to_ids(text)\n",
        "    inputs = torch.LongTensor(inputs).unsqueeze(0).to(device)\n",
        "    output = model.inference(inputs)\n",
        "\n",
        "\n",
        "print(f\"Type of output: {type(output)}\")\n",
        "print(f\"Output keys: {output.keys() if isinstance(output, dict) else None}\")\n",
        "\n",
        "\n",
        "if isinstance(output, dict) and \"model_outputs\" in output:\n",
        "    output_wav = output[\"model_outputs\"]\n",
        "else:\n",
        "    output_wav = output \n",
        "\n",
        "if isinstance(output_wav, torch.Tensor):\n",
        "    output_wav = output_wav.cpu().numpy()\n",
        "\n",
        "if output_wav.ndim > 1:\n",
        "    output_wav = output_wav.squeeze()\n",
        "\n",
        "\n",
        "ap.save_wav(output_wav, \"output.wav\")\n",
        "\n",
        "print(\"Audio saved to output.wav\")\n",
        "\n",
        "display(Audio(output_wav, rate=22050))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
